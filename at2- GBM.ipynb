{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840181cd",
   "metadata": {},
   "source": [
    "# Diferenças AdaBoost e GBM\n",
    "## AdaBoost: \n",
    "### -primeiro passo é um stump \n",
    "### -cada resposta tem um peso diferente\n",
    "### -ele atribui pesos aos classificadores e às observações, capturando assim a variação máxima nos dados.\n",
    "### -construção de tocos\n",
    "### -os pontos de dados ponderados máximos são usados para identificar as deficiências.\n",
    "\n",
    "## GBM: \n",
    "### -primeiro passo é a média do Y\n",
    "### -todas respostas tem um multiplicador em comum chamado learning_rate\n",
    "### -ele constrói árvores nos resíduos do classificador anterior, capturando assim a variação nos dados.\n",
    "### -construção de árvores\n",
    "### -os próprios gradientes identificam as deficiências"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff4f1a",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22a9617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d2de9",
   "metadata": {},
   "source": [
    "# Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c39b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "loss='squared_error').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f115c1",
   "metadata": {},
   "source": [
    "# Hyperparametros\n",
    "\n",
    "## learning_rate:\n",
    "### A taxa de aprendizado diminui a contribuição de cada árvore em learning_rate. Há uma compensação entre learning_rate e n_estimators. \n",
    "\n",
    "\n",
    "\n",
    "## n_estimators:\n",
    "### O número de estágios de reforço a serem executados. O reforço de gradiente é bastante robusto ao ajuste excessivo, portanto, um grande número geralmente resulta em melhor desempenho. \n",
    "\n",
    "\n",
    "\n",
    "## subsample:\n",
    "### A fração de amostras a ser usada para ajustar os alunos básicos individuais. \n",
    "\n",
    "\n",
    "\n",
    "## criterion:\n",
    "### A função para medir a qualidade de uma divisão. Os critérios suportados são 'friedman_mse' para o erro quadrático médio com pontuação de melhoria de Friedman, 'squared_error' para o erro quadrático médio.\n",
    "\n",
    "\n",
    "\n",
    "## min_samples_split:\n",
    "### O número mínimo de amostras necessárias para dividir um nó interno:\n",
    "\n",
    "\n",
    "\n",
    "## min_samples_leaf: \n",
    "### O número mínimo de amostras necessárias para estar em um nó folha. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36b61e",
   "metadata": {},
   "source": [
    "# Maior diferença entre GBM e Stochastic GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1115e0",
   "metadata": {},
   "source": [
    "### Especificamente, ele propôs que a cada iteração do algoritmo, um aprendiz base deveria ser ajustado em uma subamostra do conjunto de treinamento sorteado aleatoriamente sem reposição. E foi observado uma melhora na precisão"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
